MRAE - Memory-retrieved album explorer

VLM + Album

Best fit for a shared on‑device knowledge base. Each photo becomes a memory object with locally generated captions, tags, and embeddings, enabling semantic search (“sunset with two people and a dog”). This strongly demonstrates Cactus SDK, local VLM inference, and private, offline retrieval. Feasible in 24 hours: batch index camera roll, store embeddings, build a fast on‑device vector index, and ship a simple gallery UI. Meets criteria: local, usable .apk/.ipa, zero cloud.

•⁠  ⁠Stack fit: Liquid VLM for captioning, Qwen3 for text embeddings and query expansion, lightweight Smol for fast on-device tasks.

•⁠  ⁠Memory angle: persistent photo KB with query, filters, and incremental updates.

Inspired ideas:
•⁠  ⁠Semantic Photo Finding
•⁠  ⁠⁠Sketch to FInd
•⁠  ⁠⁠Find by Mood (tbd)
•⁠  ⁠⁠Learn User’s Taste in Photo Likes and Deletes
•⁠  ⁠⁠Inspiring Recommendations
•⁠  ⁠⁠AI Photo Editing (NanoBanana)
•⁠  ⁠⁠AI Video Generation
•⁠  ⁠⁠Post copy generation
•⁠  ⁠⁠Photo Graph Generation
•⁠  ⁠⁠AI Categorization

Useful ideas that we want to implement for the hackathon:
•⁠  ⁠Semantic Photo Finding (Mood) | Explore
•⁠  ⁠Tinder for Garbage collection photos/videos | 
•⁠  ⁠⁠AI Photo Editing (NanoBanana) | Creative
•⁠  ⁠⁠AI Categorisation (OCR, Embeddings) | Gallery 

Tab / Navigation bar / app bar:
Gallery, Explore, Creative, Clean, Settings
